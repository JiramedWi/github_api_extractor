{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-07T08:44:40.522559Z",
     "start_time": "2023-11-07T08:44:40.513554Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import joblib\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import model_selection\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# TODO: Preparing pre-process, \n",
    "# Load spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_tagger_for_spacy(tag):\n",
    "    # Mapping NLTK POS tags to spaCy POS tags\n",
    "    tag_dict = {'N': 'NOUN', 'V': 'VERB', 'R': 'ADV', 'J': 'ADJ'}\n",
    "    return tag_dict.get(tag, 'n')\n",
    "\n",
    "def pre_process_spacy(s):\n",
    "    doc = nlp(s)\n",
    "    s = \" \".join([token.lemma_ if token.pos_ in ['NOUN', 'VERB'] else token.text for token in doc if token.pos_ in ['NOUN', 'VERB']])\n",
    "    return s\n",
    "\n",
    "\n",
    "def pre_process_textblob(s):\n",
    "    blob = TextBlob(s)\n",
    "    # Remove stopwords\n",
    "    s = [word for word in blob.words if word not in nltk.corpus.stopwords.words('english')]\n",
    "    s = \" \".join(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def pre_process_porterstemmer(s):\n",
    "    ps = PorterStemmer()\n",
    "    s = word_tokenize(s)\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    stop_dict = {s: 1 for s in stopwords_set}\n",
    "    s = [w for w in s if w not in stop_dict]\n",
    "    s = [ps.stem(w) for w in s]\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def pre_process_lemmatizer(s):\n",
    "    s = word_tokenize(s)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    stop_dict = {s: 1 for s in stopwords_set}\n",
    "    tags = nltk.pos_tag(s)\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), tags))\n",
    "    s = [lemmatizer.lemmatize(word, tag) if tag == 'n' or tag == 'v' else None for word, tag in wordnet_tagged]\n",
    "    s = list(filter(None, s))\n",
    "    s = [w for w in s if w not in stop_dict]\n",
    "    s = ' '.join(s)\n",
    "    return s\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T08:44:40.789983Z",
     "start_time": "2023-11-07T08:44:40.522450Z"
    }
   },
   "id": "daf2505774a714e5"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Jumma/git_repo/github_api_extractor/ERAWAN/project/ESERH/resources/tsdetect/all_test_smell/df_test_semantic_smell.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m data\u001B[38;5;241m.\u001B[39mrename(columns\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle_n_body\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle_n_body_not_clean\u001B[39m\u001B[38;5;124m'\u001B[39m}, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     10\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([data, x\u001B[38;5;241m.\u001B[39mdropna()], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m y1 \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mabspath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../resources/tsdetect/all_test_smell/df_test_semantic_smell.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m y2 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(Path(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../resources/tsdetect/all_test_smell/df_issue_in_test_step.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\u001B[1;32m     14\u001B[0m y3 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(Path(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../resources/tsdetect/all_test_smell/df_code_related.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/master/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/master/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/master/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/master/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/master/lib/python3.10/site-packages/pandas/io/common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/Jumma/git_repo/github_api_extractor/ERAWAN/project/ESERH/resources/tsdetect/all_test_smell/df_test_semantic_smell.csv'"
     ]
    }
   ],
   "source": [
    "# TODO: Prepare X, Y as Same length\n",
    "filepath = Path(os.path.abspath('../resources/clean_demo.pkl'))\n",
    "x = pd.read_pickle(filepath)\n",
    "word_counts = x.str.count(' ') + 1\n",
    "\n",
    "# Check X\n",
    "data = pd.read_pickle(Path(os.path.abspath('../resources/hive_use_for_run_pre_process.pkl')))\n",
    "data = data[data['title_n_body'].notnull()]\n",
    "data.rename(columns={'title_n_body': 'title_n_body_not_clean'}, inplace=True)\n",
    "data = pd.concat([data, x.dropna()], axis=1)\n",
    "\n",
    "y1 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_test_semantic_smell.csv')))\n",
    "y2 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_issue_in_test_step.csv')))\n",
    "y3 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_code_related.csv')))\n",
    "y4 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_dependencies.csv')))\n",
    "y5 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_test_execution.csv')))\n",
    "\n",
    "\n",
    "def compare_y_to_x(dfx, dfy):\n",
    "    return dfy.loc[dfy['url'].isin(dfx['url'])]\n",
    "\n",
    "y1_to_x = compare_y_to_x(data, y1)\n",
    "y2_to_x = compare_y_to_x(data, y2)\n",
    "y3_to_x = compare_y_to_x(data, y3)\n",
    "y4_to_x = compare_y_to_x(data, y4)\n",
    "y5_to_x = compare_y_to_x(data, y5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T08:45:56.088376Z",
     "start_time": "2023-11-07T08:45:55.566476Z"
    }
   },
   "id": "8ea8a9abdf8568bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test_semantic_smell = y1_to_x['y']\n",
    "y_issue_in_test_step = y2_to_x['y']\n",
    "y_code_related = y3_to_x['y']\n",
    "y_dependencies = y4_to_x['y']\n",
    "y_test_execution = y5_to_x['y']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-07T08:44:40.807433Z"
    }
   },
   "id": "1abe4170da03b806"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_semantic_smell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m Y_train_list \u001B[38;5;241m=\u001B[39m [\u001B[43my_test_semantic_smell\u001B[49m, y_issue_in_test_step, y_code_related, y_dependencies, y_test_execution]\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Generate the text representation\u001B[39;00m\n\u001B[1;32m      4\u001B[0m text_representation \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_test_semantic_smell' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Prepare X: TF, TF-IDF, Ngram 1-3,\n",
    "# vectorizer_porter_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_porterstemmer)\n",
    "tf_vectorizer_porter_pre = CountVectorizer(preprocessor=pre_process_porterstemmer, ngram_range=(1, 3))\n",
    "tf_vectorizer_lemma_pre = CountVectorizer(preprocessor=pre_process_lemmatizer, ngram_range=(1, 3))\n",
    "tf_vectorizer_textblob_pre = CountVectorizer(preprocessor=pre_process_textblob, ngram_range=(1, 3))\n",
    "tf_vectorizer_spacy_pre = CountVectorizer(preprocessor=pre_process_spacy, ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vectorizer_porter_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_porterstemmer, ngram_range=(1, 3))\n",
    "tfidf_vectorizer_lemma_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_lemmatizer, ngram_range=(1, 3))\n",
    "tfidf_vectorizer_textblob_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_textblob, ngram_range=(1, 3))\n",
    "tfidf_vectorizer_spacy_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_spacy, ngram_range=(1, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T08:46:39.454459Z",
     "start_time": "2023-11-07T08:46:39.435777Z"
    }
   },
   "id": "d5e7015780dc1dc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Method to apply 1,0 and Log(1+X) normalization\n",
    "\n",
    "def scale_sparse_matrix(tfidf_matrix):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(tfidf_matrix.toarray())\n",
    "    return csr_matrix(x_scaled)\n",
    "\n",
    "def log_transform_tfidf(tfidf_matrix):\n",
    "    return np.log1p(tfidf_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a78220625416291"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Split80:20\n",
    "x_fit, x_test = model_selection.train_test_split(x, test_size=0.2)\n",
    "\n",
    "y_for_train_test_semantic_smell, y_for_test_test_semantic_smell = model_selection.train_test_split(y_test_semantic_smell, test_size=0.2)\n",
    "y_for_train_issue_in_test_step, y_for_test_issue_in_test_step = model_selection.train_test_split(y_issue_in_test_step, test_size=0.2)\n",
    "y_for_train_code_related, y_for_test_code_related = model_selection.train_test_split(y_code_related, test_size=0.2)\n",
    "y_for_train_dependencies, y_for_test_dependencies = model_selection.train_test_split(y_dependencies, test_size=0.2)\n",
    "y_for_train_test_execution, y_for_test_test_execution = model_selection.train_test_split(y_test_execution, test_size=0.2)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f01446af81dbd6eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_fit.info"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78194934e3645f88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Fit each X \n",
    "X_tf_train_porter = tf_vectorizer_porter_pre.fit_transform(x_fit)\n",
    "X_tf_test_porter = tf_vectorizer_porter_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_porter = tfidf_vectorizer_porter_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_porter = tfidf_vectorizer_porter_pre.transform(x_test)\n",
    "\n",
    "X_tf_train_lemma = tf_vectorizer_lemma_pre.fit_transform(x_fit)\n",
    "X_tf_test_lemma = tf_vectorizer_lemma_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_lemma = tfidf_vectorizer_lemma_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_lemma = tfidf_vectorizer_lemma_pre.transform(x_test)\n",
    "\n",
    "X_tf_train_spacy = tf_vectorizer_spacy_pre.fit_transform(x_fit)\n",
    "X_tf_test_spacy = tf_vectorizer_lemma_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_spacy = tfidf_vectorizer_spacy_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_spacy = tfidf_vectorizer_spacy_pre.transform(x_test)\n",
    "\n",
    "X_tf_train_textblob = tf_vectorizer_textblob_pre.fit_transform(x_fit)\n",
    "X_tf_test_textblob = tf_vectorizer_textblob_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_textblob = tfidf_vectorizer_textblob_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_textblob = tfidf_vectorizer_textblob_pre.transform(x_test)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aa3cc6f09c745dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_tfidf_train_porter.shape)\n",
    "print(X_tfidf_test_porter.shape)\n",
    "print(y_for_train_test_semantic_smell.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f9b93368da3952a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Prepare X: Normalization (0-1) and Log(1+x) only TFIDF\n",
    "# print(X_tf_train_porter.toarray())\n",
    "# print(X_tfidf_test_porter.toarray())\n",
    "# X_tfidf_train_porter_01, X_tfidf_test_porter_01 = scale_sparse_matrix(X_tfidf_train_porter, X_tfidf_test_porter)\n",
    "X_tfidf_train_porter_01 = scale_sparse_matrix(X_tfidf_train_porter)\n",
    "X_tfidf_train_porter_log = log_transform_tfidf(X_tfidf_train_porter)\n",
    "X_tfidf_test_porter_01 = scale_sparse_matrix(X_tfidf_test_porter)\n",
    "X_tfidf_test_porter_log = log_transform_tfidf(X_tfidf_test_porter)\n",
    "# \n",
    "X_tfidf_train_lemma_01 = scale_sparse_matrix(X_tfidf_train_lemma)\n",
    "X_tfidf_train_lemma_log = log_transform_tfidf(X_tfidf_train_lemma)\n",
    "X_tfidf_test_lemma_01 = scale_sparse_matrix(X_tfidf_test_lemma)\n",
    "X_tfidf_test_lemma_log = log_transform_tfidf(X_tfidf_test_lemma)\n",
    "# \n",
    "X_tfidf_train_spacy_01 = scale_sparse_matrix(X_tfidf_train_spacy)\n",
    "X_tfidf_train_spacy_log = log_transform_tfidf(X_tfidf_train_spacy)\n",
    "X_tfidf_test_spacy_01 = scale_sparse_matrix(X_tfidf_test_spacy)\n",
    "X_tfidf_test_spacy_log = log_transform_tfidf(X_tfidf_test_spacy)\n",
    "# \n",
    "X_tfidf_train_textblob_01 = scale_sparse_matrix(X_tfidf_train_textblob)\n",
    "X_tfidf_train_textblob_log = log_transform_tfidf(X_tfidf_train_textblob)\n",
    "X_tfidf_test_textblob_01 = scale_sparse_matrix(X_tfidf_test_textblob)\n",
    "X_tfidf_test_textblob_log = log_transform_tfidf(X_tfidf_test_textblob)\n",
    "\n",
    "X_train_list = [X_tfidf_train_porter_01, X_tfidf_train_lemma_01, X_tfidf_train_spacy_01, X_tfidf_train_textblob_01, X_tfidf_train_porter_log, X_tfidf_train_lemma_log, X_tfidf_train_spacy_log, X_tfidf_train_textblob_log]\n",
    "X_test_list = [X_tfidf_test_porter_01, X_tfidf_test_porter_log, X_tfidf_test_lemma_01, X_tfidf_test_lemma_log, X_tfidf_test_spacy_01, X_tfidf_test_spacy_log, X_tfidf_test_textblob_01, X_tfidf_test_textblob_log]\n",
    "Y_train_list = [y_for_train_test_semantic_smell, y_for_train_code_related, y_for_train_dependencies, y_for_train_test_execution, y_for_train_issue_in_test_step]\n",
    "Y_test_list = [y_for_test_test_semantic_smell, y_for_test_code_related, y_for_test_dependencies, y_for_test_test_execution, y_for_test_issue_in_test_step]\n",
    "# print(X_tfidf_train_porter_01.shape)\n",
    "# print('-------------')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7306a7a6ebc5eeef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dirname = os.path.expanduser('~')\n",
    "x_train_path = os.path.join(dirname, 'x_train')\n",
    "x_test_path = os.path.join(dirname, 'x_test')\n",
    "y_train_path = os.path.join(dirname, 'y_train')\n",
    "y_test_path = os.path.join(dirname, 'y_test')\n",
    "data_set_path = os.path.join(dirname,'data_set')\n",
    "os.makedirs(x_train_path, exist_ok=True)\n",
    "os.makedirs(x_test_path, exist_ok=True)\n",
    "os.makedirs(y_train_path, exist_ok=True)\n",
    "os.makedirs(y_test_path, exist_ok=True)\n",
    "print(dirname)\n",
    "print(data_set_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bf35a727f01399a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Set SMOTE **Problem with lib** Change to imbalance sklearn\n",
    "\n",
    "# for i, (X_train, X_test, Y_train, Y_test) in enumerate(zip(X_train_list, X_test_list, Y_train_list, Y_test_list)):\n",
    "#     X_train_var_name = [name for name, value in locals().items() if value is X_train][0]\n",
    "#     X_test_var_name = [name for name, value in locals().items() if value is X_test][0]\n",
    "#     Y_train_var_name = [name for name, value in locals().items() if value is Y_train][0]\n",
    "#     Y_test_var_name = [name for name, value in locals().items() if value is Y_test][0]\n",
    "# \n",
    "#     # Apply SMOTE\n",
    "#     smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "#     X_resampled, Y_resampled = smote.fit_resample(X_train, Y_train)\n",
    "# \n",
    "#     # Save the datasets as files\n",
    "#     joblib.dump(X_resampled, f'{x_train_path}/{X_train_var_name}_smote.pkl')\n",
    "#     joblib.dump(X_test, f'{x_test_path}/{X_test_var_name}.pkl')\n",
    "#     joblib.dump(Y_resampled, f'{y_train_path}/{Y_train_var_name}_smote.pkl')\n",
    "#     joblib.dump(Y_test, f'{y_test_path}/{Y_test_var_name}.pkl')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce2612593ed1209f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_data_combinations(X_train_list, Y_train_list, output_dir):\n",
    "    data_combinations = []  # List to store data combinations\n",
    "\n",
    "    for X_train in X_train_list:\n",
    "        X_train_var_name = [name for name, value in locals().items() if value is X_train][0]\n",
    "        for Y_train in Y_train_list:\n",
    "            Y_train_var_name = [name for name, value in locals().items() if value is Y_train][0]\n",
    "            # Apply SMOTE\n",
    "            smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "            X_resampled, Y_resampled = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "            # Store the combination as a dictionary\n",
    "            data_combination = {\n",
    "                \"X_train\": X_resampled,\n",
    "                \"Y_train\": Y_resampled,\n",
    "            }\n",
    "            data_combinations.append(data_combination)\n",
    "\n",
    "            # Save the data combination to a file (optional)\n",
    "            output_file = f\"{output_dir}/data_combination_{X_train_var_name}_{Y_train_var_name}.pkl\"\n",
    "            joblib.dump(data_combination, output_file)\n",
    "\n",
    "    return data_combinations\n",
    "\n",
    "data_combinations = generate_data_combinations(X_train_list, Y_train_list, data_set_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7c4d4003d80b7a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Set SMOTE need to split train test?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15655599d0e4c7df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: ML Model: GBM\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "# what we should set on setting?\n",
    "# model = gbm_model.fit(X_tfidf_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3661d69ec754e62d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: ML Model: Cross_validation, Metric\n",
    "# precision = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "# n_jobs=-2, scoring='precision_macro')\n",
    "# recall = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "# n_jobs=-2, scoring='recall_macro')\n",
    "# f1_cv_score = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "# n_jobs=-2, scoring='f1_macro')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dc9c6d375de6614"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d86ca6e65b8f2289"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recall"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d32de251afd31736"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_cv_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85856826b56941de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: ML Model: GBM\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "# what we should set on setting?\n",
    "# res_model = gbm_model.fit(X_res, y_res)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f95d70c783bfbe97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
