{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.386108Z",
     "start_time": "2023-11-02T04:40:31.367935Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import joblib\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import model_selection\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# TODO: Preparing pre-process, \n",
    "# Load spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_tagger_for_spacy(tag):\n",
    "    # Mapping NLTK POS tags to spaCy POS tags\n",
    "    tag_dict = {'N': 'NOUN', 'V': 'VERB', 'R': 'ADV', 'J': 'ADJ'}\n",
    "    return tag_dict.get(tag, 'n')\n",
    "\n",
    "def pre_process_spacy(s):\n",
    "    doc = nlp(s)\n",
    "    s = \" \".join([token.lemma_ if token.pos_ in ['NOUN', 'VERB'] else token.text for token in doc if token.pos_ in ['NOUN', 'VERB']])\n",
    "    return s\n",
    "\n",
    "\n",
    "def pre_process_textblob(s):\n",
    "    blob = TextBlob(s)\n",
    "    # Remove stopwords\n",
    "    s = [word for word in blob.words if word not in nltk.corpus.stopwords.words('english')]\n",
    "    s = \" \".join(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def pre_process_porterstemmer(s):\n",
    "    ps = PorterStemmer()\n",
    "    s = word_tokenize(s)\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    stop_dict = {s: 1 for s in stopwords_set}\n",
    "    s = [w for w in s if w not in stop_dict]\n",
    "    s = [ps.stem(w) for w in s]\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def pre_process_lemmatizer(s):\n",
    "    s = word_tokenize(s)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    stop_dict = {s: 1 for s in stopwords_set}\n",
    "    tags = nltk.pos_tag(s)\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), tags))\n",
    "    s = [lemmatizer.lemmatize(word, tag) if tag == 'n' or tag == 'v' else None for word, tag in wordnet_tagged]\n",
    "    s = list(filter(None, s))\n",
    "    s = [w for w in s if w not in stop_dict]\n",
    "    s = ' '.join(s)\n",
    "    return s\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.672461Z",
     "start_time": "2023-11-02T04:40:31.377155Z"
    }
   },
   "id": "daf2505774a714e5"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# TODO: Prepare X, Y as Same length\n",
    "filepath = Path(os.path.abspath('../resources/clean_demo.pkl'))\n",
    "x = pd.read_pickle(filepath)\n",
    "word_counts = x.str.count(' ') + 1\n",
    "\n",
    "# Check X\n",
    "data = pd.read_pickle(Path(os.path.abspath('../resources/hive_use_for_run_pre_process.pkl')))\n",
    "data = data[data['title_n_body'].notnull()]\n",
    "data.rename(columns={'title_n_body': 'title_n_body_not_clean'}, inplace=True)\n",
    "data = pd.concat([data, x.dropna()], axis=1)\n",
    "\n",
    "y1 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_test_semantic_smell.csv')))\n",
    "y2 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_issue_in_test_step.csv')))\n",
    "y3 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_code_related.csv')))\n",
    "y4 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_dependencies.csv')))\n",
    "y5 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_test_execution.csv')))\n",
    "\n",
    "\n",
    "def compare_y_to_x(dfx, dfy):\n",
    "    return dfy.loc[dfy['url'].isin(dfx['url'])]\n",
    "\n",
    "y1_to_x = compare_y_to_x(data, y1)\n",
    "y2_to_x = compare_y_to_x(data, y2)\n",
    "y3_to_x = compare_y_to_x(data, y3)\n",
    "y4_to_x = compare_y_to_x(data, y4)\n",
    "y5_to_x = compare_y_to_x(data, y5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.707130Z",
     "start_time": "2023-11-02T04:40:31.675090Z"
    }
   },
   "id": "8ea8a9abdf8568bd"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "y_test_semantic_smell = y1_to_x['y']\n",
    "y_issue_in_test_step = y2_to_x['y']\n",
    "y_code_related = y3_to_x['y']\n",
    "y_dependencies = y4_to_x['y']\n",
    "y_test_execution = y5_to_x['y']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.711612Z",
     "start_time": "2023-11-02T04:40:31.708907Z"
    }
   },
   "id": "1abe4170da03b806"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# TODO: Prepare X: TF, TF-IDF, Ngram 1-3,\n",
    "# vectorizer_porter_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_porterstemmer)\n",
    "tf_vectorizer_porter_pre = CountVectorizer(preprocessor=pre_process_porterstemmer, ngram_range=(1, 3))\n",
    "tf_vectorizer_lemma_pre = CountVectorizer(preprocessor=pre_process_lemmatizer, ngram_range=(1, 3))\n",
    "tf_vectorizer_textblob_pre = CountVectorizer(preprocessor=pre_process_textblob, ngram_range=(1, 3))\n",
    "tf_vectorizer_spacy_pre = CountVectorizer(preprocessor=pre_process_spacy, ngram_range=(1, 3))\n",
    "\n",
    "\n",
    "tfidf_vectorizer_porter_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_porterstemmer, ngram_range=(1, 3))\n",
    "tfidf_vectorizer_lemma_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_lemmatizer, ngram_range=(1, 3))\n",
    "tfidf_vectorizer_textblob_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_textblob, ngram_range=(1, 3))\n",
    "tfidf_vectorizer_spacy_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_spacy, ngram_range=(1, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.762318Z",
     "start_time": "2023-11-02T04:40:31.746101Z"
    }
   },
   "id": "d5e7015780dc1dc1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# TODO: Method to apply 1,0 and Log(1+X) normalization\n",
    "\n",
    "def scale_sparse_matrix(tfidf_matrix):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(tfidf_matrix.toarray())\n",
    "    return csr_matrix(x_scaled)\n",
    "\n",
    "def log_transform_tfidf(tfidf_matrix):\n",
    "    return np.log1p(tfidf_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.767291Z",
     "start_time": "2023-11-02T04:40:31.763515Z"
    }
   },
   "id": "2a78220625416291"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Split80:20\n",
    "x_fit, x_test = model_selection.train_test_split(x, test_size=0.2)\n",
    "\n",
    "y_for_train_test_semantic_smell, y_for_test_test_semantic_smell = model_selection.train_test_split(y_test_semantic_smell, test_size=0.2)\n",
    "y_for_train_issue_in_test_step, y_for_test_issue_in_test_step = model_selection.train_test_split(y_issue_in_test_step, test_size=0.2)\n",
    "y_for_train_code_related, y_for_test_code_related = model_selection.train_test_split(y_code_related, test_size=0.2)\n",
    "y_for_train_dependencies, y_for_test_dependencies = model_selection.train_test_split(y_dependencies, test_size=0.2)\n",
    "y_for_train_test_execution, y_for_test_test_execution = model_selection.train_test_split(y_test_execution, test_size=0.2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.771596Z",
     "start_time": "2023-11-02T04:40:31.768069Z"
    }
   },
   "id": "f01446af81dbd6eb"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Series.info of 1164    hive23509 mapjoin assertionerror capacity must...\n1115    hive23797 throw exception when no metastore fo...\n593     hive25372 advance write id for all the ddls ad...\n115     hive26683 sum windowing function returns wrong...\n825     revert hive24624 repl load should detect the c...\n                              ...                        \n1181                     hive16787 fix itests in branch22\n1111                        hive23483 remove dynamicserde\n276     hive26280 copy more data into completedcompact...\n1122    hive23444 concurrent acid direct inserts may f...\n282     hive22670 arrayindexoutofboundsexception when ...\nName: title_n_body, Length: 917, dtype: object>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fit.info"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:40:31.776472Z",
     "start_time": "2023-11-02T04:40:31.773228Z"
    }
   },
   "id": "78194934e3645f88"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# TODO: Fit each X \n",
    "\n",
    "X_tf_train_porter = tf_vectorizer_porter_pre.fit_transform(x_fit)\n",
    "X_tf_test_porter = tf_vectorizer_porter_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_porter = tfidf_vectorizer_porter_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_porter = tfidf_vectorizer_porter_pre.transform(x_test)\n",
    "\n",
    "X_tf_train_lemma = tf_vectorizer_lemma_pre.fit_transform(x_fit)\n",
    "X_tf_test_lemma = tf_vectorizer_lemma_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_lemma = tfidf_vectorizer_lemma_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_lemma = tfidf_vectorizer_lemma_pre.transform(x_test)\n",
    "\n",
    "X_tf_train_spacy = tf_vectorizer_spacy_pre.fit_transform(x_fit)\n",
    "X_tf_test_spacy = tf_vectorizer_lemma_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_spacy = tfidf_vectorizer_spacy_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_spacy = tfidf_vectorizer_spacy_pre.transform(x_test)\n",
    "\n",
    "X_tf_train_textblob = tf_vectorizer_textblob_pre.fit_transform(x_fit)\n",
    "X_tf_test_textblob = tf_vectorizer_textblob_pre.transform(x_test)\n",
    "\n",
    "X_tfidf_train_textblob = tfidf_vectorizer_textblob_pre.fit_transform(x_fit)\n",
    "X_tfidf_test_textblob = tfidf_vectorizer_textblob_pre.transform(x_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:41:11.878550Z",
     "start_time": "2023-11-02T04:40:31.779048Z"
    }
   },
   "id": "5aa3cc6f09c745dc"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 51005)\n",
      "(230, 51005)\n",
      "(917,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf_train_porter.shape)\n",
    "print(X_tfidf_test_porter.shape)\n",
    "print(y_for_train_test_semantic_smell.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:41:11.886187Z",
     "start_time": "2023-11-02T04:41:11.868708Z"
    }
   },
   "id": "3f9b93368da3952a"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_list\n"
     ]
    }
   ],
   "source": [
    "# TODO: Prepare X: Normalization (0-1) and Log(1+x) only TFIDF\n",
    "# print(X_tf_train_porter.toarray())\n",
    "# print(X_tfidf_test_porter.toarray())\n",
    "# X_tfidf_train_porter_01, X_tfidf_test_porter_01 = scale_sparse_matrix(X_tfidf_train_porter, X_tfidf_test_porter)\n",
    "X_tfidf_train_porter_01 = scale_sparse_matrix(X_tfidf_train_porter)\n",
    "X_tfidf_train_porter_log = log_transform_tfidf(X_tfidf_train_porter)\n",
    "X_tfidf_test_porter_01 = scale_sparse_matrix(X_tfidf_test_porter)\n",
    "X_tfidf_test_porter_log = log_transform_tfidf(X_tfidf_test_porter)\n",
    "# \n",
    "X_tfidf_train_lemma_01 = scale_sparse_matrix(X_tfidf_train_lemma)\n",
    "X_tfidf_train_lemma_log = log_transform_tfidf(X_tfidf_train_lemma)\n",
    "X_tfidf_test_lemma_01 = scale_sparse_matrix(X_tfidf_test_lemma)\n",
    "X_tfidf_test_lemma_log = log_transform_tfidf(X_tfidf_test_lemma)\n",
    "# \n",
    "X_tfidf_train_spacy_01 = scale_sparse_matrix(X_tfidf_train_spacy)\n",
    "X_tfidf_train_spacy_log = log_transform_tfidf(X_tfidf_train_spacy)\n",
    "X_tfidf_test_spacy_01 = scale_sparse_matrix(X_tfidf_test_spacy)\n",
    "X_tfidf_test_spacy_log = log_transform_tfidf(X_tfidf_test_spacy)\n",
    "# \n",
    "X_tfidf_train_textblob_01 = scale_sparse_matrix(X_tfidf_train_textblob)\n",
    "X_tfidf_train_textblob_log = log_transform_tfidf(X_tfidf_train_textblob)\n",
    "X_tfidf_test_textblob_01 = scale_sparse_matrix(X_tfidf_test_textblob)\n",
    "X_tfidf_test_textblob_log = log_transform_tfidf(X_tfidf_test_textblob)\n",
    "\n",
    "X_train_list = [X_tfidf_train_porter_01, X_tfidf_train_lemma_01, X_tfidf_train_spacy_01, X_tfidf_train_textblob_01, X_tfidf_train_porter_log, X_tfidf_train_lemma_log, X_tfidf_train_spacy_log, X_tfidf_train_textblob_log]\n",
    "X_test_list = [X_tfidf_test_porter_01, X_tfidf_test_porter_log, X_tfidf_test_lemma_01, X_tfidf_test_lemma_log, X_tfidf_test_spacy_01, X_tfidf_test_spacy_log, X_tfidf_test_textblob_01, X_tfidf_test_textblob_log]\n",
    "Y_train_list = [y_for_train_test_semantic_smell, y_for_train_code_related, y_for_train_dependencies, y_for_train_test_execution, y_for_train_issue_in_test_step]\n",
    "Y_test_list = [y_for_test_test_semantic_smell, y_for_test_code_related, y_for_test_dependencies, y_for_test_test_execution, y_for_test_issue_in_test_step]\n",
    "# print(X_tfidf_train_porter_01.shape)\n",
    "# print('-------------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T05:26:42.055918Z",
     "start_time": "2023-11-02T05:26:39.563417Z"
    }
   },
   "id": "7306a7a6ebc5eeef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dirname = os.path.dirname('/home/jiramed_withun')\n",
    "filename = os.path.join(dirname, '/x_y_for_train')\n",
    "print(dirname)\n",
    "print(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7bf35a727f01399a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Set SMOTE **Problem with lib** Change to imbalance sklearn\n",
    "# def oversampling(x,y):\n",
    "#     oversampler = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "#     x_resampled, y_resampled = oversampler.fit_resample(x, y)\n",
    "#     return x_resampled, y_resampled\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(X_train_list)):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    Y_train = Y_train_list[i]\n",
    "    Y_test = Y_test_list[i]\n",
    "    X_train_var_name = [name for name, value in locals().items() if value is X_train][0]\n",
    "    X_test_var_name = [name for name, value in locals().items() if value is X_test][0]\n",
    "    Y_train_var_name = [name for name, value in locals().items() if value is Y_train][0]\n",
    "    Y_test_var_name = [name for name, value in locals().items() if value is Y_test][0]\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, Y_resampled = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "    # Save the datasets as files\n",
    "    joblib.dump(X_resampled, f'{X_train_var_name}_smote.pkl')\n",
    "    joblib.dump(X_test, f'{X_test_var_name}.pkl')\n",
    "    joblib.dump(Y_resampled, f'{X_train_var_name}_smote.pkl')\n",
    "    joblib.dump(Y_test, f'{Y_test_var_name}.pkl')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:20:19.885086Z",
     "start_time": "2023-11-02T04:20:19.883323Z"
    }
   },
   "id": "ce2612593ed1209f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Set SMOTE need to split train test?\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:20:19.889933Z",
     "start_time": "2023-11-02T04:20:19.885136Z"
    }
   },
   "id": "15655599d0e4c7df"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tfidf_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m gbm_model \u001B[38;5;241m=\u001B[39m GradientBoostingClassifier()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# what we should set on setting?\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m gbm_model\u001B[38;5;241m.\u001B[39mfit(\u001B[43mX_tfidf_train\u001B[49m, y_train)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_tfidf_train' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: ML Model: GBM\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "# what we should set on setting?\n",
    "model = gbm_model.fit(X_tfidf_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:20:20.314108Z",
     "start_time": "2023-11-02T04:20:19.890415Z"
    }
   },
   "id": "3661d69ec754e62d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: ML Model: Cross_validation, Metric\n",
    "precision = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "n_jobs=-2, scoring='precision_macro')\n",
    "recall = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "n_jobs=-2, scoring='recall_macro')\n",
    "f1_cv_score = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "n_jobs=-2, scoring='f1_macro')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:20:20.318632Z",
     "start_time": "2023-11-02T04:20:20.315317Z"
    }
   },
   "id": "8dc9c6d375de6614"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T04:20:20.316799Z"
    }
   },
   "id": "d86ca6e65b8f2289"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recall"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T04:20:20.318497Z"
    }
   },
   "id": "d32de251afd31736"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_cv_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T04:20:20.319876Z"
    }
   },
   "id": "85856826b56941de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: ML Model: GBM\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "# what we should set on setting?\n",
    "# res_model = gbm_model.fit(X_res, y_res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T04:20:20.320469Z"
    }
   },
   "id": "f95d70c783bfbe97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
