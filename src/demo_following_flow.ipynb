{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-02T04:18:27.030999Z",
     "start_time": "2023-11-02T04:18:25.152619Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import smote_variants as sv\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# TODO: Preparing pre-process, \n",
    "# Load spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_tagger_for_spacy(tag):\n",
    "    # Mapping NLTK POS tags to spaCy POS tags\n",
    "    tag_dict = {'N': 'NOUN', 'V': 'VERB', 'R': 'ADV', 'J': 'ADJ'}\n",
    "    return tag_dict.get(tag, 'n')\n",
    "\n",
    "def pre_process_spacy(s):\n",
    "    doc = nlp(s)\n",
    "    s = \" \".join([token.lemma_ if token.pos_ in ['NOUN', 'VERB'] else token.text for token in doc if token.pos_ in ['NOUN', 'VERB']])\n",
    "    return s\n",
    "\n",
    "\n",
    "def pre_process_textblob(s):\n",
    "    blob = TextBlob(s)\n",
    "    # Remove stopwords\n",
    "    s = [word for word in blob.words if word not in nltk.corpus.stopwords.words('english')]\n",
    "    s = \" \".join(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def pre_process_porterstemmer(s):\n",
    "    ps = PorterStemmer()\n",
    "    s = word_tokenize(s)\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    stop_dict = {s: 1 for s in stopwords_set}\n",
    "    s = [w for w in s if w not in stop_dict]\n",
    "    s = [ps.stem(w) for w in s]\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def pre_process_lemmatizer(s):\n",
    "    s = word_tokenize(s)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    stop_dict = {s: 1 for s in stopwords_set}\n",
    "    tags = nltk.pos_tag(s)\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), tags))\n",
    "    s = [lemmatizer.lemmatize(word, tag) if tag == 'n' or tag == 'v' else None for word, tag in wordnet_tagged]\n",
    "    s = list(filter(None, s))\n",
    "    s = [w for w in s if w not in stop_dict]\n",
    "    s = ' '.join(s)\n",
    "    return s\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T11:38:39.076727Z",
     "start_time": "2023-10-26T11:38:38.798797Z"
    }
   },
   "id": "daf2505774a714e5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# TODO: Prepare X, Y as Same length\n",
    "filepath = Path(os.path.abspath('../resources/clean_demo.pkl'))\n",
    "x = pd.read_pickle(filepath)\n",
    "word_counts = x.str.count(' ') + 1\n",
    "\n",
    "# Check X\n",
    "data = pd.read_pickle(Path(os.path.abspath('../resources/hive_use_for_run_pre_process.pkl')))\n",
    "data = data[data['title_n_body'].notnull()]\n",
    "data.rename(columns={'title_n_body': 'title_n_body_not_clean'}, inplace=True)\n",
    "data = pd.concat([data, x.dropna()], axis=1)\n",
    "\n",
    "y1 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_test_semantic_smell.csv')))\n",
    "y2 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_issue_in_test_step.csv')))\n",
    "y3 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_code_related.csv')))\n",
    "y4 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_dependencies.csv')))\n",
    "y5 = pd.read_csv(Path(os.path.abspath('../resources/tsdetect/all_test_smell/df_test_execution.csv')))\n",
    "\n",
    "\n",
    "def compare_y_to_x(dfx, dfy):\n",
    "    return dfy.loc[dfy['url'].isin(dfx['url'])]\n",
    "\n",
    "y1_to_x = compare_y_to_x(data, y1)\n",
    "y2_to_x = compare_y_to_x(data, y2)\n",
    "y3_to_x = compare_y_to_x(data, y3)\n",
    "y4_to_x = compare_y_to_x(data, y4)\n",
    "y5_to_x = compare_y_to_x(data, y5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T11:38:39.119154Z",
     "start_time": "2023-10-26T11:38:39.078868Z"
    }
   },
   "id": "8ea8a9abdf8568bd"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Series.info of 0       0\n1       0\n2       0\n3       1\n4       0\n       ..\n1179    1\n1180    0\n1181    0\n1182    0\n1183    1\nName: y, Length: 1147, dtype: int64>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_to_x.info\n",
    "y_test_semantic_smell = y1_to_x['y']\n",
    "y_issue_in_test_step = y2_to_x['y']\n",
    "y_code_related = y3_to_x['y']\n",
    "y_dependencies = y4_to_x['y']\n",
    "y_test_execution = y5_to_x['y']\n",
    "y_test_semantic_smell.info"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T11:41:47.280271Z",
     "start_time": "2023-10-26T11:41:47.269560Z"
    }
   },
   "id": "1abe4170da03b806"
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "# TODO: Prepare X: TF-IDF, Ngram 1, Normalization MinMax(0,1)\n",
    "# vectorizer_porter_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_porterstemmer)\n",
    "tfidf_vectorizer_porter_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_porterstemmer, ngram_range=(1, 2, 3))\n",
    "tfidf_vectorizer_lemm_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_lemmatizer, ngram_range=(1, 2, 3))\n",
    "tfidf_vectorizer_textblob_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_textblob, ngram_range=(1, 2, 3))\n",
    "tfidf_vectorizer_spacy_pre = TfidfVectorizer(use_idf=True, preprocessor=pre_process_spacy, ngram_range=(1, 2, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:15.901184Z",
     "start_time": "2023-09-05T08:07:15.895435Z"
    }
   },
   "id": "d5e7015780dc1dc1"
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Split80:20\n",
    "x_fit, x_test = model_selection.train_test_split(x, test_size=0.2)\n",
    "\n",
    "y_for_train_test_semantic_smell, y_for_test_test_semantic_smell = model_selection.train_test_split(y_test_semantic_smell, test_size=0.2)\n",
    "y_for_train_issue_in_test_step, y_for_test_issue_in_test_step = model_selection.train_test_split(y_issue_in_test_step, test_size=0.2)\n",
    "y_for_train_code_related, y_for_test_code_related = model_selection.train_test_split(y_code_related, test_size=0.2)\n",
    "y_for_train_dependencies, y_for_test_dependencies= model_selection.train_test_split(y_dependencies, test_size=0.2)\n",
    "y_for_train_test_execution, y_for_test_test_execution= model_selection.train_test_split(y_test_execution, test_size=0.2)\n",
    "\n",
    "X_tfidf_vector = tfidf_vectorizer_lemm_pre.fit(x_fit)\n",
    "\n",
    "X_tfidf_train = X_tfidf_vector.transform(x_fit)\n",
    "X_tfidf_test = X_tfidf_vector.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:18.942441Z",
     "start_time": "2023-09-05T08:07:15.899647Z"
    }
   },
   "id": "f01446af81dbd6eb"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 20938)\n",
      "(917,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf_train.shape)\n",
    "print(y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:18.946383Z",
     "start_time": "2023-09-05T08:07:18.942640Z"
    }
   },
   "id": "3f9b93368da3952a"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "# TODO: Prepare X,Y: Set SMOTE **Problem with lib**\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = SMOTE(random_state=0)\n",
    "# # X_for_res = np.array(X_tfidf_train.todense())\n",
    "# X_for_res = pd.DataFrame(X_tfidf_train.toarray(), columns=tfidf_vectorizer_lemm_pre.get_feature_names_out())\n",
    "# X_res, y_res = sm.fit_resample(X_tfidf_train, np.array(y_train))\n",
    "# X_res_test, y_res_test = sm.fit_resample(X_tfidf_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:18.949830Z",
     "start_time": "2023-09-05T08:07:18.945544Z"
    }
   },
   "id": "ce2612593ed1209f"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:18.950219Z",
     "start_time": "2023-09-05T08:07:18.948171Z"
    }
   },
   "id": "42f6b5fa813e7702"
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "# TODO: Prepare X: Normalization MinMax(0,1) **Problem**\n",
    "# scaler = MinMaxScaler() \n",
    "# standardized_tfidf_matrix = scaler.fit_transform(X_tfidf_train)\n",
    "# standardized_tfidf_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:18.953713Z",
     "start_time": "2023-09-05T08:07:18.950815Z"
    }
   },
   "id": "91096eb92cc705b2"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "# TODO: ML Model: GBM\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "# what we should set on setting?\n",
    "model = gbm_model.fit(X_tfidf_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:19.890500Z",
     "start_time": "2023-09-05T08:07:18.953960Z"
    }
   },
   "id": "3661d69ec754e62d"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "# TODO: ML Model: Cross_validation, Metric\n",
    "precision = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "n_jobs=-2, scoring='precision_macro')\n",
    "recall = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "n_jobs=-2, scoring='recall_macro')\n",
    "f1_cv_score = model_selection.cross_val_score(model, X_tfidf_train, y_train, cv=5,\n",
    "n_jobs=-2, scoring='f1_macro')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:29.962781Z",
     "start_time": "2023-09-05T08:07:19.892090Z"
    }
   },
   "id": "8dc9c6d375de6614"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.43902439, 0.62007011, 0.5491453 , 0.52752682, 0.62814465])"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:29.967739Z",
     "start_time": "2023-09-05T08:07:29.963973Z"
    }
   },
   "id": "d86ca6e65b8f2289"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.48189522, 0.53762198, 0.53865579, 0.49747475, 0.52447552])"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:29.972099Z",
     "start_time": "2023-09-05T08:07:29.967072Z"
    }
   },
   "id": "d32de251afd31736"
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.43246753, 0.52583139, 0.51004995, 0.45601842, 0.52502781])"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_cv_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:29.976236Z",
     "start_time": "2023-09-05T08:07:29.972229Z"
    }
   },
   "id": "85856826b56941de"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "# TODO: ML Model: GBM\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "# what we should set on setting?\n",
    "# res_model = gbm_model.fit(X_res, y_res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T08:07:29.980210Z",
     "start_time": "2023-09-05T08:07:29.975425Z"
    }
   },
   "id": "f95d70c783bfbe97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
